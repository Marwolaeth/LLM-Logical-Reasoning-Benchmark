---
title: "Оценка логических способностей LLM с помощью vitals и Ollama: Глубокое погружение"
author: "Andrej Pawluczenko"
format: docx
---

# Введение: Зачем оценивать LLM (и почему это важно даже для "шмурдиков")

В мире больших языковых моделей (LLM) постоянно появляются новые инструменты и подходы к их оценке. Сегодня мы хотим поделиться нашим опытом использования относительно новой и очень перспективной R-библиотеки `vitals` для систематической оценки логических способностей различных LLM, запущенных локально через Ollama.

## Новая библиотека `vitals`: Ваш помощник в оценке LLM

Библиотека `vitals` (разработанная командой `tidyverse`) — это мощный фреймворк для оценки производительности LLM. Она позволяет стандартизировать процесс тестирования, собирать метрики и сравнивать различные модели и промпты. Это значительно упрощает и ускоряет эксперименты, позволяя сосредоточиться на анализе результатов, а не на рутинной организации тестов.

## LLM и Логика: Не все так просто, как кажется

Несмотря на впечатляющие способности LLM в генерации текста, суммаризации и даже креативном письме, их логические рассуждения часто остаются камнем преткновения. Как известно даже самым "шмурдикам" (тем, кто только начинает свой путь в мире LLM), модели могут "галлюцинировать", выдавать противоречивые ответы или просто не справляться с задачами, требующими строгого логического вывода. Наша цель — количественно оценить, насколько хорошо различные модели справляются с задачей логического следования (Natural Language Inference, NLI) при разных условиях промптинга.

# Наш Логический Тест: От Посылки к Гипотезе

Для оценки логических способностей мы используем классическую задачу NLI, где модели предлагается пара предложений: **посылка** (premise) и **гипотеза** (hypothesis). Задача модели — определить логическую связь между ними.

## Набор данных

Мы используем набор данных `voinarovsky_test.csv`. Этот файл содержит пары "посылка-гипотеза" с заранее размеченной логической связью. В нашем случае, мы ожидаем от модели один из двух ответов:
*   `entailment`: если гипотеза логически следует из посылки (т.е., если посылка истинна, то гипотеза *обязательно* истинна).
*   `not_entailment`: во всех остальных случаях, включая противоречие (contradiction) и нейтральную связь (neutral).

Точное количество примеров в полном наборе данных составляет ..., а его характеристики включают в себя ... (например, разнообразие тематик, сложность синтаксиса). Для ускорения тестирования мы используем подвыборку из `r N_SAMPLES` примеров.

## Задача

Основная задача для LLM — проанализировать предоставленную пару "посылка-гипотеза" и выдать строго один из двух ответов: `'entailment'` или `'not_entailment'`.

# Процесс Оценки: Шаг за Шагом

Наш процесс оценки организован в несколько логических шагов, каждый из которых отражен в нашем R-скрипте.

## 1. Загрузка необходимых библиотек

Первым делом, мы загружаем все необходимые R-пакеты: `vitals`, `ellmer` (для взаимодействия с LLM), `tibble`, `dplyr`, `readr` (для работы с данными).

```r
if (!require(vitals)) {
    devtools::install_github('tidyverse/vitals', build = TRUE)
}

library(vitals)
library(ellmer)
library(tibble)
library(dplyr)
library(readr)
```

## 2. Определение промптов: Искусство общения с LLM

Один из ключевых аспектов оценки LLM — это влияние промпта на качество ответов. Мы определили несколько различных системных промптов, каждый из которых имеет свою стратегию:

*   `simple`: Самый лаконичный промпт, предполагающий, что модель уже "знает" задачу.
*   `extreme_human`: Промпт, который "гуманизирует" задачу, предлагая модели представить себя на собеседовании.
*   `expanded`: Детализированный промпт с подробными определениями `entailment` и `not_entailment`.
*   `no-reasoning`: Промпт, явно запрещающий модели генерировать любые дополнительные рассуждения.
*   `professional`: Использует профессиональную терминологию (NLI), знакомую моделям, обученным на NLP-данных.
*   `explicit-binary`: Подчеркивает бинарную природу выбора.
*   `formal`: Максимально формализованный промпт с использованием логических символов (`P`, `H`, `⊨`).
*   `expert-default`: Стандартный промпт, с которого мы начали.

Все эти промпты хранятся в `tibble` под названием `system_prompts` и затем сохраняются в файл `prompts/logic-prompts.RData` для дальнейшего использования.

```r
# ... (код определения system_prompts) ...
save(system_prompts, file = 'prompts/logic-prompts.RData')

system_prompt <- "Вы — эксперт по логическим умозаключениям. Ваша задача — определить логическую связь между посылкой и гипотезой. Отвечайте только одним из следующих вариантов: 'entailment' (если гипотеза логически следует из посылки) или 'not_entailment' (если гипотеза не следует из посылки)."
user_prompt <- "Посылка: {premise}\nГипотеза: {hypothesis}\n\nКакова логическая связь?"
```

## 3. Загрузка и подготовка данных

Мы загружаем наш тестовый набор данных, выбираем нужные столбцы (`premise`, `hypothesis`, `label2`), переименовываем `label2` в `target` и создаем столбец `input`, который будет содержать комбинацию посылки и гипотезы, отформатированную согласно `user_prompt`.

```r
data_path <- 'data/voinarovsky_test.csv'
voinarovsky <- read_csv2(data_path)

df <- voinarovsky |>
    select(premise, hypothesis, target = label2) |>
    mutate(input = glue::glue(user_prompt) |> as.character()) |>
    select(input, target)
```

## 4. Схема вывода данных: Структурированный ответ

Одной из мощных возможностей `ellmer` (и `vitals` через `ellmer`) является поддержка структурированного вывода. Мы определяем `schema`, которая гарантирует, что модель вернет ответ в виде JSON-объекта с полем `answer`, которое может принимать только значения `'entailment'` или `'not_entailment'`. Это критически важно для автоматической оценки.

```r
schema <- type_object(
    answer = type_enum(
        c('entailment', 'not_entailment'),
        description = 'Следует ли гипотеза из посылки (entailment) или нет',
        required = TRUE
    )
)
```

## 5. Задание для оценки: `vitals` в действии

`vitals` работает с концепцией `Task` (задачи), которая объединяет набор данных, решатель (solver) и оценщик (scorer).

### 5.1. Пример использования `vitals`

В коде приведен небольшой пример оценки простых математических задач, чтобы продемонстрировать базовый функционал `vitals`.

### 5.2. Наша функция для структурированного вывода: `generate_structured`

Поскольку мы хотим, чтобы модели возвращали структурированный JSON-ответ, мы создали вспомогательную функцию `generate_structured`. Эта функция обертывает `ellmer::parallel_chat_structured`, которая позволяет получать ответы от LLM в заданном формате. Она принимает `solver_chat` (объект `Chat` из `ellmer`) и `schema` и возвращает функцию, которую `vitals` может использовать как `solver`.

```r
generate_structured <- function(solver_chat = NULL, schema = NULL) {
    chat <- solver_chat
    schm <- schema
    
    function(inputs, schema = schm, ..., solver_chat = chat) {
        # ... (проверки и логика вызова ellmer::parallel_chat_structured) ...
        res <- ellmer::parallel_chat_structured(
            chat = ch,
            prompts = as.list(inputs),
            type = schema,
            ...
        )
        
        list(result = as.list(as.character(res$answer)), 
             solver_chat = list(ch))
    }
}
```

## 6. Создание и оценка задачи: Автоматизация тестирования

### 6.1. Функция `model_logic_test`: Сердце нашей оценки

Мы создали функцию `model_logic_test`, которая инкапсулирует весь процесс оценки для одной модели и одного системного промпта. Она:
*   Инициализирует объект `chat_ollama` для указанной модели и системного промпта.
*   Создает `vitals::Task`, используя наш подготовленный набор данных, `generate_structured` как решатель и `detect_exact()` как оценщик (который проверяет точное совпадение ответа модели с целевым значением).
*   Запускает оценку с помощью `tsk$eval()`.
*   Собирает метрики, время выполнения и информацию о промпте.

```r
model_logic_test <- function(
    model_name,
    dataset,
    system_prompt,
    task_name = NULL,
    epochs = 1L
) {
    # ... (логика создания Chat, Task и выполнения eval) ...
    return(tsk$eval(view = FALSE, epochs = epochs))
}
```

### 6.2. Цикл оценки: Тестируем все комбинации

Самая интересная часть — это цикл оценки. Мы итерируемся по всем определенным системным промптам и по всем доступным моделям Ollama, запуская `model_logic_test` для каждой комбинации. Результаты (точность, время, использованные токены) собираются в список `results_list`.

```r
load(file = 'prompts/logic-prompts.RData')
vitals::vitals_log_dir_set("./logs")

results_list <- list()

for (prompt_obj in purrr::transpose(as.list(system_prompts))) {
    prompt_name <- prompt_obj$name
    
    for (model_name in ollama_models) {
        tictoc::tic()
        evaluation_results <- model_logic_test(
            model_name,
            dataset = df_eval,
            system_prompt = prompt_obj$prompt,
            task_name = glue::glue(
                'logic-{model_name}-{prompt_name}-{nrow(df_eval)}-x1'
            )
        )
        stopwatch <- tictoc::toc()
        
        evaluation_results[['time']] <- stopwatch$toc - stopwatch$tic
        evaluation_results[['prompt_type']] <- prompt_name
        
        results_list[[model_name]] <- evaluation_results
    }
}
```

# Анализ и Сравнение Результатов (Заготовка)

После завершения всех оценок мы агрегируем полученные данные в `tibble` `comparison_df`.

```r
comparison_df <- tibble(
    model = character(),
    prompt = character(),
    accuracy = numeric(),
    time = numeric(),
    tokens_input = numeric(),
    tokens_output = numeric()
)

for (model_name in names(results_list)) {
    current_results <- results_list[[model_name]]
    
    cost <- current_results$get_cost()
    
    comparison_df <- comparison_df %>%
        add_row(
            model = model_name,
            prompt = current_results$prompt,
            accuracy = current_results$metrics, # Здесь будут метрики, например, accuracy
            time = current_results$time,
            tokens_input = cost$input,
            tokens_output = cost$output
        )
}

comparison_df <- comparison_df |>
    mutate(tokens_total = tokens_input + tokens_output) |>
    left_join(
        select(system_prompts, prompt = name, prompt_length = nchar)
    )
print(comparison_df)
```

## Что мы будем анализировать?

Мы планируем детально проанализировать следующие аспекты:
*   **Точность (Accuracy)**: Какая модель и какой промпт дают наилучшую точность в определении логического следования?
*   **Влияние промпта**: Как различные стратегии промптинга (лаконичные, гуманизированные, детализированные, формальные) влияют на производительность моделей? Есть ли "оптимальный" промпт для логических задач?
*   **Скорость и стоимость**: Сколько времени занимает оценка каждой модели с каждым промптом? Каково потребление токенов (входных и выходных) для разных моделей и промптов?
*   **Длина промпта**: Есть ли корреляция между длиной промпта и точностью/скоростью?

## Как мы будем представлять результаты?

Для наглядности мы будем использовать различные визуализации:
*   **Столбчатые диаграммы** для сравнения точности моделей по типам промптов.
*   **Точечные диаграммы** для анализа зависимости точности от длины промпта или времени выполнения.
*   **Таблицы** с агрегированными метриками для быстрого обзора.

Мы также планируем поделиться полными логами оценки для тех, кто захочет провести более глубокий анализ.

# Заключение

Оценка LLM — это не просто академический интерес, а практическая необходимость для тех, кто строит приложения на их основе. Библиотека `vitals` предоставляет мощный и гибкий инструмент для этой цели. Наши эксперименты с логическими задачами и различными промптами помогут нам лучше понять сильные и слабые стороны текущих LLM и выработать рекомендации по их эффективному использованию в задачах, требующих точных логических рассуждений. Оставайтесь с нами, чтобы узнать результаты!